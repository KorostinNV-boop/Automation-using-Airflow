{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06910dd",
   "metadata": {
    "id": "e06910dd"
   },
   "source": [
    "# Автоматизация с помощью Airflow\n",
    "\n",
    "- Автор: Коростин Никита\n",
    "- Дата: 01.11.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149134c",
   "metadata": {
    "id": "4149134c"
   },
   "source": [
    "## Цели и задачи проекта\n",
    "\n",
    "Сервис предоставляет доступ к контенту разных форматов, включая текст, аудио и не только. В этом проекте вы построите пайплайн в Airflow, который будет запускать PySpark-скрипт для обработки данных и создания витрин. Эти витрины помогут команде сервиса быстрее и проще готовить отчёты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728986ea",
   "metadata": {
    "id": "728986ea"
   },
   "source": [
    "## Описание данных\n",
    "\n",
    "Таблица `bookmate.audition` содержит данные об активности пользователей и включает столбцы:\n",
    "\n",
    "* `audition_id` — уникальный идентификатор сессии чтения или прослушивания;\n",
    "\n",
    "* `puid` — идентификатор пользователя;\n",
    "\n",
    "* `usage_platform_ru` — название платформы, с помощью которой пользователь взаимодействует с контентом;\n",
    "\n",
    "* `msk_business_dt_str` — дата и время события (строка, часовой пояс — МСК);\n",
    "\n",
    "* `app_version` — версия приложения;\n",
    "\n",
    "* `adult_content_flg` — значение, которое показывает, был ли контент для взрослых (`True` или `False`);\n",
    "\n",
    "* `hours` — длительность сессии чтения или прослушивания в часах;\n",
    "\n",
    "* `hours_sessions_long` — длительность длинных сессий в часах;\n",
    "\n",
    "* `kids_content_flg` — значение, которое показывает, был ли это детский контент (`True` или `False`);\n",
    "\n",
    "* `main_content_id` — идентификатор основного контента;\n",
    "\n",
    "* `usage_geo_id` — идентификатор географического местоположения пользователя.\n",
    "\n",
    "Таблица `bookmate.content` включает столбцы:\n",
    "\n",
    "* `main_content_id` — идентификатор основного контента;\n",
    "\n",
    "* `main_author_id` — идентификатор основного автора контента;\n",
    "\n",
    "* `main_content_type` — тип контента: аудио, текст или другой;\n",
    "\n",
    "* `main_content_name` — название контента;\n",
    "\n",
    "* `main_content_duration_hours` — длительность контента в часах;\n",
    "\n",
    "* `published_topic_title_list` — список жанров или тем контента.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecaf95",
   "metadata": {
    "id": "4fecaf95"
   },
   "source": [
    "## Содержимое проекта\n",
    "\n",
    "Проект предполагает несколько шагов:\n",
    "\n",
    "1. Написать Spark-код — вы подключитесь к своему хранилищу данных и укажете, куда сохранять результат.\n",
    "\n",
    "2. Создать DAG — он будет запускать Spark-код. В DAG вы опишете задачи с помощью `PythonOperator` и `DataprocCreatePysparkJobOperator`, настроите зависимости и получите агрегированные данные для первых бизнес-выводов.\n",
    "\n",
    "3. Запустить Airflow — именно он будет управлять вашим пайплайном."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WGKIjeRdzV0U",
   "metadata": {
    "id": "WGKIjeRdzV0U"
   },
   "source": [
    "## 1. Написание Spark-кода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QjNj1KvlzXu2",
   "metadata": {
    "id": "QjNj1KvlzXu2"
   },
   "source": [
    "Ваши данные для подключения к DBeaver:\n",
    "*   Имя пользователя — (скрыто)\n",
    "*   Пароль — (скрыто)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d21d6",
   "metadata": {
    "id": "6f7d21d6"
   },
   "source": [
    "В коде ниже приведён написанный Spark-скрипт. Ваша задача — правильно указать данные для подключения к вашему хранилищу: порты, параметры кластера ClickHouse и путь, куда будут записываться агрегаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05faf34b",
   "metadata": {
    "id": "05faf34b"
   },
   "outputs": [],
   "source": [
    "# filename=my_spark_job.py\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "\n",
    "# Создаём Spark-сессию и при необходимости добавляем конфигурации\n",
    "spark = SparkSession.builder.appName(\"myAggregateTest\").config(\"fs.s3a.endpoint\", \"storage.yandexcloud.net\").getOrCreate()\n",
    "\n",
    "# Указываем порт и параметры кластера ClickHouse\n",
    "jdbcPort = 8443\n",
    "jdbcHostname = \"(скрыто)\"\n",
    "username = \"(скрыто)\"\n",
    "jdbcDatabase = \"playground_\" + username\n",
    "jdbcUrl = f\"jdbc:clickhouse://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}?ssl=true\"\n",
    "\n",
    "# Получаем аргумент из Airflow\n",
    "my_date = sys.argv[1].replace('-', '_')\n",
    "\n",
    "# Считываем исходные данные за нужную дату\n",
    "df = spark.read.csv(f\"s3a://da-plus-dags/script_bookmate/data_{my_date}/audition_content.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# Строим агрегат по пользователям\n",
    "result_df = df.groupBy(\"puid\").agg(\n",
    "    F.countDistinct(\"audition_id\").alias(\"audition_count\"),\n",
    "    F.avg(\"hours\").alias(\"avg_hours\")\n",
    ")\n",
    "\n",
    "result_df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", \"(скрыто)\") \\\n",
    "    .option(\"dbtable\", \"bookmate_user_aggregate\") \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6-FZswEsB2P",
   "metadata": {
    "id": "p6-FZswEsB2P"
   },
   "source": [
    "В результате будет создан файл с названием, указанным в первой строке. Этот файл можно будет запустить с помощью Airflow, но сначала понадобится настроить DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e82e2",
   "metadata": {
    "id": "956e82e2"
   },
   "source": [
    "## 2. Создание DAG\n",
    "\n",
    "Теперь, когда Spark-код готов, нужно создать DAG, который будет его запускать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vMNkNpPYsKS-",
   "metadata": {
    "id": "vMNkNpPYsKS-"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Создайте «каркас» нового DAG для вашего проекта. DAG должен запускаться каждый день начиная с 1 января 2025 года. При этом запускать DAG за пропущенные даты не нужно.\n",
    "\n",
    "Используйте менеджер контекста `with ... as dag:` — так все задачи будут корректно привязаны к DAG. После конструкции пока напишите только `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67956690",
   "metadata": {
    "id": "67956690"
   },
   "outputs": [],
   "source": [
    "# filename=bookmate_dag.py\n",
    "\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.sensors.s3_key_sensor import S3KeySensor\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "class PysparkJobOperator(DataprocCreatePysparkJobOperator):\n",
    "    template_fields = (\"cluster_id\", \"args\",)\n",
    "\n",
    "DAG_ID = \"audition_content_analysis\"\n",
    "\n",
    "with DAG(\n",
    "    dag_id=DAG_ID,\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MtrlsIUGsVX1",
   "metadata": {
    "id": "MtrlsIUGsVX1"
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Теперь добавьте проверку входного файла. DAG не должен стартовать, пока в S3 не появится файл с данными за нужную дату.\n",
    "\n",
    "Для решения используйте сенсор `S3KeySensor`. Он должен проверять наличие файла каждые 5 минут и ждать максимум час. В качестве аргумента для параметра `bucket_name` укажите строку `\"da-plus-dags\"`.\n",
    "\n",
    "Файл называется `audition_content.csv`, но в имени папки должна быть дата запуска в формате `YYYY_MM_DD`. Например, для 5 января 2025 путь будет таким: `script_bookmate/data_2025_01_05/audition_content.csv`. Путь к данным  должен быть аргументом для параметра `bucket_key` в `S3KeySensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c46cb",
   "metadata": {
    "id": "b55c46cb"
   },
   "outputs": [],
   "source": [
    "# filename=bookmate_dag.py\n",
    "\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.sensors.s3_key_sensor import S3KeySensor\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "class PysparkJobOperator(DataprocCreatePysparkJobOperator):\n",
    "    template_fields = (\"cluster_id\", \"args\",)\n",
    "\n",
    "DAG_ID = \"audition_content_analysis\"\n",
    "\n",
    "    wait_for_input = S3KeySensor(\n",
    "        task_id='wait_for_input',\n",
    "        bucket_name=\"da-plus-dags\",\n",
    "        bucket_key=\"script_bookmate/data_{{ ds.replace('-', '_') }}/audition_content.csv\",\n",
    "        aws_conn_id='s3',\n",
    "        poke_interval=300, \n",
    "        timeout=3600,   \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jY8aAW9Gsb_U",
   "metadata": {
    "id": "jY8aAW9Gsb_U"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Создайте задачу для запуска Spark-скрипта через Airflow. Укажите путь к файлу, который вы создали на первом шаге проекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SE_morl2seZq",
   "metadata": {
    "id": "SE_morl2seZq"
   },
   "outputs": [],
   "source": [
    "# filename=bookmate_dag.py\n",
    "\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "    run_pyspark = PysparkJobOperator(\n",
    "        task_id=\"run_pyspark_job\",\n",
    "        name=\"audition_content_analysis\",\n",
    "        cluster_id=\"(скрыто)\",\n",
    "        main_python_file_uri=\"s3a://da-plus-dags/script_bookmate/my_spark_job.py\",\n",
    "        args=[\"{{ ds }}\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0zdUDAY1shG5",
   "metadata": {
    "id": "0zdUDAY1shG5"
   },
   "source": [
    "### Задание 4\n",
    "\n",
    "Теперь соберите все фрагменты вместе:\n",
    "\n",
    "* Опишите DAG с нужными параметрами.\n",
    "\n",
    "* Добавьте сенсор для ожидания входного файла.\n",
    "\n",
    "* Добавьте Spark-задачу для запуска скрипта.\n",
    "\n",
    "* Настройте зависимости так, чтобы Spark-задача запускалась только после появления файла."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ieU9hC2Aznes",
   "metadata": {
    "id": "ieU9hC2Aznes"
   },
   "source": [
    "Ваши данные для подключения к Airflow:\n",
    "*   IP — (скрыто)\n",
    "*   Имя пользователя — (скрыто)\n",
    "*   Пароль — (скрыто)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uCA8ULn6sjOq",
   "metadata": {
    "id": "uCA8ULn6sjOq"
   },
   "outputs": [],
   "source": [
    "# filename=bookmate_dag.py\n",
    "\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.sensors.s3_key_sensor import S3KeySensor\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "class PysparkJobOperator(DataprocCreatePysparkJobOperator):\n",
    "    template_fields = (\"cluster_id\", \"args\",)\n",
    "\n",
    "DAG_ID = \"audition_content_analysis\"\n",
    "\n",
    "with DAG(\n",
    "    dag_id=DAG_ID,\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    # 1) Ждём появления входного файла в S3\n",
    "    wait_for_input = S3KeySensor(\n",
    "        task_id='wait_for_input',\n",
    "        bucket_name=\"da-plus-dags\",\n",
    "        bucket_key=\"script_bookmate/data_{{ ds.replace('-', '_') }}/audition_content.csv\",\n",
    "        aws_conn_id='s3',\n",
    "        poke_interval=300, \n",
    "        timeout=3600,    \n",
    "    )\n",
    "\n",
    "    run_pyspark = PysparkJobOperator(\n",
    "        task_id=\"run_pyspark_job\",\n",
    "        name=\"audition_content_analysis\",\n",
    "        cluster_id=\"(скрыто)\",\n",
    "        main_python_file_uri=\"s3a://da-plus-dags/script_bookmate/my_spark_job.py\",\n",
    "        args=[\"{{ ds }}\"]\n",
    "    )\n",
    "\n",
    "    wait_for_input >> run_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953e84c",
   "metadata": {
    "id": "6953e84c"
   },
   "source": [
    "## 3. Запуск Airflow\n",
    "\n",
    "Теперь можно переходить к запуску. Нажмите кнопку «Проверить», подождите 5 минут и снова нажмите её. Вам будут показаны данные для входа в веб-интерфейс Airflow. В интерфейсе найдите ваш DAG и запустите его.\n",
    "\n",
    "Проверьте, что DAG выполнился, а результат соответствует ожиданиям. Если всё получилось — поздравляем, проект завершён!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
